5-feb-2018
==========

Learning Python

Python is designed to be readable, and has support for Object-oriented 
programming and functional programming, which are software-reuse mechanisms.

Python code is typically 1/3 to 1/5 the size of equivalent C++ or Java code.
This means less typing, less debugging, and less maintaining. It doesn't
need to compile so it can run immediately.

Porting between Mac and Windows is trivial.

A powerful advantage is having so many 3rd-party tools. Even free NumPy has been 
described as being more pwerful than Matlab. 

Python seems to "fit your brain", in that the language is consistent and 
its features interact in limited ways, and arise naturally from core concepts.

Python adopts a minimalist approach. There is usually just one obvious way to
approach a coding task and few less obvious alternatives. Explicit is better
than implicit - simple is better than complex.

Python was designed to get more done with less effort. It is deliberately
optimized for speed of development. 

Typing "import this" at the Python interpreter prompt gives "The Zen of Python".

Python is a general-purpose programming language that blends procedural,
functional, and object-oriented paradigms. 

One downside to python is that it's not as fast as C and C++, since it's not
a fully-compiled-low-level language. 

Standard implementations of Python convert source code to byte code, and then
interpret the byte code. Byte code allows for portability, since it is platform-
independent. It is however not compiled all the way down to binary machine code,
thus why it'll be slower than fully compiled languages like C. PyPy can achieve
10x to 100x speedup by compiling further, but it's a separate and alternative
implementation.

Oftentimes, when needing speed, Python interpreter can hand off tasks to 
compiled C code - so the Python script can run at C speed. Python's speed-of-
development is often more important than speed-of-execution loss, especially
as computers get faster - but sometimes you do need fast, like numeric
programming and animation. 

If we need speed, we can split the parts that need it into compiled extensions.
Compiled extensions won't be talked much about here, but NumPy does it.

Another downside is Python changing and evolving so much. It's hard to teach,
which is why the book is so big. But it is still simpler than its alternatives
like Java, etc...

The "libraries included" approach can also have drawbacks. This includes relying
on libraries which may not be well maintained, or not understanding completely
a particular tool. It can be nice for beginners, though, looking to be 
productive.

It also gives falls into some open source traps - such as the personal 
preference of the few triumphs over the common usage of the many.

Python is very stable and robust, and has been around for over 25 years, and has
a large community, with the BDFL (Benevolent Dictator for Life) at the helm 
(Guido van Rossum). (He's the creator.)

Python is written in portable ANSI C, and compiles and runs on virtually every
platform: Linux/Unix, Windows, Mac, Cray supercomputers, Palm OS, Symbian,
Windows Mobile, Android, iOS, etc...

Python's "powers":

 - Dynamic typing: Python keeps track of the types of objects the program
 uses as it runs - no such thing as type or variable declaration anywhere.

 - Automatic memory management - allocates objects and reclaims (garbage
 collection) them - most can grow and shrink on demand. 

 - Includes tools like modules, classes, and exceptions.

 - Built-in object types - provides commonly used data structures - like lists,
 dictionaries, and strings.

 - Built in tools: Standard operations, like joining collections slicing,
 sorting, mapping, etc...

Python is named after Monty Python - not the snake. So there might be some
joke references throughout the code.

The author believes that freedom of expression is great for art, but lousy
for engineering. It's harder to read and maintain if the programmer is too
unique or ticky. A code written with too much freedom is often easier to
write from scratch than to maintain.


6-feb-2018
==========

Python is a programming language, but it's also a software package called an
interpreter. The interpreter reads the program an carries out the instructions.
It's a layer of software logic between the code and the hardware.

Depending on how it's ran, the Python interpreter can be a C program, a set of
Java classes, or otherwise. 

A Python interpreter must be installed on the computer.

Python program files end in .py, but technically speaking, only files that need
to be imported need to have this extension. Regardless, all python scripts and 
files are given the .py extension for consistency.

When a python program is executed, it is first converted to byte code. Byte code
is a lower-level representation of the source code. Byte code is ran faster than
source code. This byte code is stored with a .pyc extension. 

Before Python3.2, the byte code is saved with .pyc extension in the same 
directory as the source code files. In Python3.2 and later, these .pyc files
are stored in a new subdirectory called __pycache__

The byte code is recreated if the source code changes, and this is 
monitored through the timestamp. Not recreating the byte code saves the time in
the compilation step. If python is being ran with a different version, then
the byte code is also recreated.

If python can't write the byte code to the machine, for instance if it doesn't
have the permission, then it will generate it in memory and it will be discarded
when the program exits.

To run a program, all you really need are the byte code files - and these can
be shipped around as "frozen binaries" if included with the PVM (described 
below).

Byte code is only saved for files that are imported - not for top level files
that are only ran as scripts. It's an import optimization.

Byte code is never saved for code typed at the interactive prompt.

Once the byte code is loaded from .pyc files, it is shipped off for execution 
to the Python Virtual Machine (PVM). The PVM just runs through the byte code 
instructions.

Most of this stuff is hidden from the programmer.

Compared to C/C++, there's no build or "make" step - the code run immediately 
after it is written. Also, byte code is not binary machine code, which are 
instructions for an Intel chip for instance. Byte code is a Python-specific 
representation. It's also why Python is slower than C/C++, but still faster 
than traditional interpreted languages (because of the internal compile step).

The systems that compile and execute the source code in Python are the same. 
This is different than languages like C/C++. It makes the development cycle
faster - no need to precompile and link.

The eval and exec built-ins can run string containing Python code - it allows 
for changes to the Python program on the fly without having to compile the 
whole system. There is only runtime, and not compile time. Everything happens
as the program is running. 

The execution model just described is not a requirement of Python, and there 
are a few variants. The most common are CPython, Jython, IronPython, Stackless,
and PyPy. All of them implement the same Python language but are executed 
differently. The one used in this book is CPython, which is really the standard.
The other implementations have specific purposes, such as direct access to Java 
(Jython) or .NET (IronPython).

CPython is coded in portable ANSI C language. Because it is standard, it tends 
to be the fastest, more complete, and more up-to-date. 

Jython allows .py text files to compile to Java byte code to be shipped to
the JVM. Jython makes python code feel like a true Java program at runtime.

Stackless Python allows for concurrency and provides for efficient 
multiprocessing options. The microthreads that Stackless adds are an efficient 
and lightweight alternative to Python's standard multitasking tools such as 
threads and processes. 

PyPy is focused on performance, and has a fast Python JIT (just-in-time) 
compiler. It includes Stackless Python's systems to support massive concurrency. 
It translates portions of the byte code all the way to binary code for faster 
execution. It does this as the program is running, not as a precompiled step. 
It keeps track of the data types of the objects that the program processes, and 
the program runs faster and faster as it executes. It can even take up less 
memory. It runs most CPython code C extension modules generally need to be 
recompiled. There is also some non-traditional garbage collection where files 
may need to be manually flushed. It's claimed to be a 5.7x speedup over CPython.
It can sometimes run as fast as C code, and occasionally faster. 

(Looking on the internet, NumPy is pretty fast and I wouldn't need to bother 
with PyPy when I'm doing NumPy numberic stuff.)

There are methods to get "frozen binaries", so that you can ship your program 
around, but are not true compilers. For Windows, there is py2exe, and for 
Mac OSx there is py2app. It's not the same as a true compiler - they run byte 
code through a virtual machine, and run at the same speed as the original 
source files. 

7-feb-2018
==========

The Python interpreter prints the result after each command. For instance, 
you can just type "2 ** 8", instead of "print(2 ** 8)". The code is executed 
immediately after pressing the Enter key. 

At the Python interactive prompt, if you do a compound statement (like
multiline), then pressing Enter twice runs the statement (or submitting a blank 
statement). This means you can't cut and paste multiple lines of code - unless 
the code includes blank lines after each compound statement. It's better to 
run this type of code in a file. 

You write your code into files, called modules. It's also refer to as a program,
which is a series of precoded statements, intended for repeated execution. 

The following is stream redirection and the output lines are saved in a file:

% python script1.py > saveit.txt 

When importing, Python omits the ".py" extension and the directory path, 
like so:

$ import script1 

To make a Unix-style executable script, start the script off with a hashbang 
followed by the path to the Python interpreter, e.g.,

#!/usr/local/bin/python 

Then give it executable priviliges, and you can run it in an executable way. 
This is a special thing that you can do on Unix. Unix looks for the firt line 
of a script with the #! pattern to find an interpreter for running the program. 
The usual naming convention for executable is not to have a .py extension, 
to make it clear that no other modules will import this file, and that it is 
also an executable. 

To avoid hardcoding the path to the Python interpreter, you can use the env 
program which locates the Python interpreter according to the system search 
path settings - usually by looking at directories listed in the PATH environment 
variable:

!#/usr/bin/env python 

This actually makes it more portable, and is actually recommended - some 
platforms could install Python elsewhere. However, it assumes that env will 
live on the same path everywhere, which isn't always the case (sbin instead of 
bin).

No special code or syntax is required to make a file a module - just that the 
extension needs to be ".py". 

The typical Python architecture is a top-level script that runs line by line 
and and import tools from other modules files, which may in turn import tools 
from other module files, etc... 

Import only runs once per session - they are to expensive to repeat, since 
they find files, compile them to byte code, and run the code. 

>>> import script1

Running the import again is possible, by calling the reload function in the 
imp standard library:

>>> from imp import reload 
>>> reload(script1)

The from statement copies a name out of a module, so you can use it as if the 
module was defined in the current script (e.g., you don't have to write: 
imp.reload(script1))

This allows for editting new code on the fly. 

8-feb-2018
==========

Whereas import is a statement, reload is a function. Reload requires that import 
has worked successfully before. Before Python 3.X, reload used be a built-in 
function but now it is a module in the imp standard library. 

Names loaded with from are not updated by reload - only with import or 
model.attribute.

A module is mostly a package of variables names, known as a namespace. The 
names within that package are called attributes. An attribute is a variable
name attached to a specific object, like a module. 

The dot expression syntax, object.attribute, lets you fetch any attribute 
attached to an object. 

If you don't want to use "object.attribute" but maybe just "attribute" only,
then you can do: 

>>> from myfile import title
>>> title

"from" is just like import, but copes a module's attribuets so that they become
simple variables in the current script. So "title" becomes a variable, instead 
of using myfile.title which is an attribute reference. 

While import can run the whole script, from only takes the specified attributes.

The dir function shows all the names available inside a module as a list.

Builtin attributes shown by the dir function lead and trail with "__", like:
'__builtin__' or '__file__'. This will be explained further later. They are 
predefined and ahve special meaning. 

Python programs are made of multiple module files linked together by import statements. One file cannot see other names in another file unless it's imported. Each file is a namespace. Names in one file cannot clash with those in another.

"from" causes names to be overwritten. It also "defeats the purpose" of namespaces. Some people are hardcore and only use import.

Using reload purely for updating code in place can get out of hand - it only reloads the module in question, but not the imports that that module needs to reload.

Another way to run code is through exec, which runs the code anew - and it doesn import the module. For instance:

>>> exec(open('script1.py').read())

But it has the potential to silently overwrite variables currently in use. It's a poor namespace best practice. If you're going to use exec, it's so much typing that it's just better to run the script at the command line.

IDLE is the standard and included Python integrated development environment (IDE). It's a play off the name of a Monty Python member, Eric Idle.

This launches the IDLE:

$ python -m idlelib.idle 

You can use python -i to run the program in interactive mode. 

10-feb-2018
===========

Chapter 4

Everything is an object in a Python script. Objects are pieces of memory, with values and sets of associated operations. 

Unlike some other languages like C/C++, Python ahs some powerful built-in objects that are ready to use, instead of implementing objects that have to deal with memory or writing setters and getters.

Here are 4 reasons why built-in objects are cool:

  - They make programs easy to write. We are given powerful tools liks lists and dictionaries.
  - They are components of extensions. Objects implemented by you are often built on top of built-in object types such as lists and dictionaries.
  - They are more efficient than custom data structures. They are usually implemented in C for speed and use opt-mized data structure algorithms. 
  - They are a standard part of the language.

Literals are expressions that generate objects. For instance, 'spam' is a literal expression that generates and returns a string. 

Lists are collections of ordered objects, and dictionaries store objects by key. Both of these types can be nested and can grow or shrink on demand. 

Core data types are those built into the Python language, such as Numbers, Strings, Lists, Dictionaries, Tuples, Files, Sets, Booleans, None, etc... 

Once you create an object, you bind its operation on the set for all time. For instance, only string operations can work on a string. 

Python is a dynamically typed language, which means that the language automatically keeps track of the types instead of requiring declaration code. However, it is strongly typed in the sense that once the object is created, there is a constraint that only the operatations that are valid for its type can be performed. 

Numbers: Operations include +, -, /, *, and **, where ** is the exponential operation. Python 3.X integer type automatically includes extra precision for large numbers when needed, as opposed to Python 2.X where a separate long integer type handles large numbers. 

str and len work on many object types, and this will be elaborated upon further throughout the book. 

In older version of python, before Python 2.7 and Python 3.1, the result of a value could look odd because it's being shown in full precision, i.e.,

>>> 3.1415 * 2
6.2830000000000004

It's a display issue. It's being shown as an object's as-code repr. If we convert it to str, it will be more user-friendly. The latest versions display themselves more intelligently:

>>> 3.1415 * 2
6.283

String can store textual information, like a name, but can also store an arbitrary collection of bytes, such as an image file's contents. They are a sequence, which maintains left-to-right order, and their items are stored and detched by their relative positions. Strings are sequences of one-character strings.

Indexes are coded as offets from the start, and start from 0. 

Python variables never need to be declared ahead of time, and is created when a value is assigned to it. It can be assigned any type of object, and is replaced with its value when it shows up in an expression.

Positive indexes count from the left, and negative count from the right.
S[-1] gets the last item from the end, and S[-2] gets the 2nd to last item in the end. Another way to look at it is that the negative index is added to the string's length. 

Slicing is taking from index a to index b, not including b, and returns a new string. For instance:
S = 'Spam'. Then S[1:3] = 'pa'. [a,b)

In slice, the left bound defaults to 0, and the right bound defaults to length of the string. For instance,

S[1:] = 'pam'
S = 'Spam'
S[0:3] = 'Spa'
S[:3] = 'Spa'
S[:-1] = 'Spa'
S[:] = 'Spam'

The + operation is makes use of a property called polymorphism, where it can mean different things for different objects. For instance, for numbers it is addition, and for strings it is concatentation. This helps for much of the conciseness and flexibility in the code. 

Strings are immutable in Python, meaning that they cannot be changed in place after they are created. You can never overwrite values of immutable objects. For instance, if you try S[0] = 'z', you will get an error. 

An object is either immutable or not. Immutable objects of core types include numbers, strings, and tuples. Mutable objects include lists, dictionaries, and sets. They can be changed in place, as can most new objects that we create with classes. Immutability guarantees that an object remains constant throughout the program. 

You can change text-based data in place if you expand it into a list of individual characters and join it back together, or use the newer bytearray type available in Python 2.6, 3.0 and later.

>>> B = bytearray(b'spam')
>>> B.extend(b'eggs')
>>> B
bytearray(b'spameggs')
>>> B.decode()
'spameggs'
>>> B[1] = ord('x')
>>> B.decode()
'sxameggs'

bytearray supports only text whose characters are at most 8-bits wide (aka ASCII). Bytearray is a special hybrid of immutable bytes strings.

Strings have their own operations, known as methods. Methods are functions that are attached to and act upon a specific object, which are triggered with a call expression. 

find returns the offset of the passed-in string, or -1 if it's not there.
replace changes the pass-in string and replaces with another passed-in string.

Both of the above methods return new strings as results, since strings are immmutable. 

Other useful string methods:

split, upper, lower, isalpha, isdigit, rstrip (removes whitespace, including new line, on the right side)

The following string expressions work for Python 3.1+:

>>> '%s, eggs, and %s' % ('spam', 'SPAM!')
'spam, eggs, and SPAM!'
>>> '{0}, eggs and {1}'.format('spam', 'SPAM!')
'spam, eggs and SPAM!'
>>> '{}, eggs, and {}'.format('spam', 'SPAM!')
'spam, eggs, and SPAM!'

Formatting is rich in features, and we'll postpone it for later in the book. But you can do fancy things with numbers:

>>> '{:,.2f}'.format(296999.2567)
'296,999.26'
>>> '%.2f | %+05d' % (3.14159, -42)
'3.14 | -0042'

As a rule of thumb, generic operations that work on multiple types are usually built-in functions, such as len(x) or X[0], while type-specific operations show up as method calls, like aString.upper().

For more details about an object, you can use dir:

>>> dir(x)
['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace','rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']

The double underscores '__' represent the implementation of the string object and are available to support customization. We can overload operators.

For example, __add__ is what really performs concatenation. Python maps the first of the following to the second internally. These two are the same:

>>> S = 'Spam'
>>> S + 'NI!'
'SpamNI!'
>>> S.__add__('NI!')
'SpamNI!'

Leading and trailing double underscores are Python conventions for implementation details. The names without underscores are callable methods on string objects. 

To see what the methods from dir do, you can use the help function:

Help on built-in function replace:

>>> help(S.replace)
replace(...) method of builtins.str instance
    S.replace(old, new[, count]) -> str

    Return a copy of S with all occurrences of substring
    old replaced by new.  If the optional argument count is
    given, only the first count occurrences are replaced.

help is one of a handful of interfaces to a system of code that ships with Python known as PyDoc - a tool for extracting documentation from objects.

It's better to ask help on a specific method, and not the object itself.

>>> help(S)
No Python documentation found for 'Spam'.
Use help() to get the interactive help utility.
Use help(str) for help on the str class.

Special characters can be represented as backslash escape sequences, which Python displays in \xNN hexadecimal escape notation. 

Python can do single quotes or double quotes - apparently most programmers prefer single quotes. 

Using three of such quotes (either single or double) allows for multiline string literals. The new lines are added wherever the line breaks occur. 

There are also raw string literals, that turn off blackslash escapes, which can be useful for directory paths: r'C:\text\new'

Python comes with full Unicode support. In Python 3.X, the str string handles Unicode text, including ASCII which is a simple kind of Unicode. It also has a distinct bytes string type that represents raw byte values, which includes media and encoded text. 

Bytes don't apply to Unicode because some encodings include character code points that are too large for a byte. Even simple 7-bit ASCII text is not stored one byte per character under some encodings and memory storage schemes.

Python 3.X support non-ASCII characters with \x hexadecimal and short \u and long \U Unicode escapes, and file-wide encodings. Here's an example of a non-ASCII character coded three ways in 3.X:

>>> 'sp\xc4\u00c4\U000000c4m'
'spÄÄÄm'

In Python 3.X, normal strings are Unicode, and byte strings have to be called out. In Python 2.X, normal strings are byte strings, and Unicode has to be called out. In Python 3.X, Unicode strings are never allowed to mix with byte strings without explicit conversion - this was not so in Python 2.X.

Str (Unicode) gets encoded to bytes. Bytes get decoded to str (unicode) - Python 3.X.

Text is encoded to bytes when stored in a file, and decoded to characters (aka code points) when read back into memory. When it is loaded, it is usually processed as strings in decoded form only. 

This means that files are content-specific in Python 3.X - text files implement named encodings adn accept and return str strings, and binary fiels deal with bytes strings for raw binary data. Python 2.X has a special codecs module to handle Unicode type in normal fiels content, which is in str bytes.

None of the string object's own methods support pattern-based matching. To do that, we need to import the re module. It has analogous calls for searching, splitting, and replacement, but can use patterns to specify substrings. 

Lists are like arrays in other languages, but are more powerful. They have no fixed type constraint - for instance, a single list can contain eleemnts of three different object types. They also have no fied size. 

Lists are mutable so they can be changed in place instead of creating a new one.

Python core data types support nesting, and can be nested in any combination and as deeply as desired. For instance, making a matrix can be a list of lists. For serious number crunching though, it's better to use NumPy and SciPy systems, which can store and process large matrixes. NumPy has been said to turn Python into a more powerful Matlab system.

List comprehension expressions are a powerful way to process structures like a matrix. Suppose M:

 >>> M = [[1, 2, 3],
...      [4, 5, 6],
...      [7, 8, 9]]
>>> M
[[1, 2, 3], [4, 5, 6], [7, 8, 9]]
>>> M[1]
[4, 5, 6]
>>> [row[1] for row in M]
[2, 5, 8]

List comprehensions derive from set notation, and built a new list. They are surrounded by square brackets to let you know they are returning a list. The previous command says "give me row[1] for each row in Matrix M as a new list". In other words, "give me the 2nd item (index 1) for each row (list) in list M as a new list".

List comprehensions can be more complicated:

>>> [row[1] for row in M if row[1] % 2 == 0]
[2, 8]

List comprehensions can be used to tierate over any iterable object.

Getting the diagonal:
>>> [M[i][i] for i in [0,1,2]]
[1, 5, 9] 

In more recent Python versions, like the one I'll likely be using, comprehension syntax isn't limited to just creating lists. When enclosed in paranthesis, they can be used to create generators that produce results on demand. For instance:

>>> G = (sum(row) for row in M)
>>> next(G)
6
>>> next(G)
15
>>> next(G)
24

The map built-in generates results of running items through a function one at a time. The following sums each row through map, and the results are returned as a list:

>>> list(map(sum, M))
[6, 15, 24]

Comprehension syntax can be used to create sets:

>>> {sum(row) for row in M}
{24, 6, 15}

and dictionaries:

>>> {i:sum(M[i]) for i in [0,1,2]}
{0: 6, 1: 15, 2: 24}
>>> {i:sum(M[i]) for i in range(3)}
{0: 6, 1: 15, 2: 24}

To summarize, comprehension syntax can be used in the following objects and the enclosed characters represent what type it is:

() => generators
[] => lists
{} => sets
{:} => dictionaries

Dictionaries are not sequences, but mappings. Mappings are collections of objects, but are stored by key instead of relative position. Dictionaries are mutable. Keys tend to be more mnemonic, or when the collection's items are named or labeled. 

Indexing a dictionary by key is the fastest way to code a search in Python. 

There are a few days to create dictionaries:

1)
>>> dict(name='Bob', job='dev', age=40)
{'name': 'Bob', 'job': 'dev', 'age': 40}

2)
>>> dict(zip(['name', 'job', 'age'], ['Bob', 'dev', 40]))
{'name': 'Bob', 'job': 'dev', 'age': 40}

3)
>>> D = {}
>>> D['name']='Bob'
>>> D['job']='dev'
>>> D['age']=40
>>> D
{'name': 'Bob', 'job': 'dev', 'age': 40}

Dictionaries can be nested also. 

If lists are part of a dictionary, the list can be appended since it is a separate piece of memory from the dictionary that contains it. 

Nesting allows the flexibility of core data types. Doing something similar in a low-level language like C would be tedious. We would have to declare and link everything together, and then worry about memory clean-up. In Python, we don't have to worry about creating space or cleaning up as we go. 

To access a key in a dictionary, it's best to test for existence of such a key first:

>>> 'f' in D 
or
>>> if not 'f' in D:
        print('missing')

If there we want to return a default value if the key doesn't exist, we can use the get method:

>>> D.get('x',0)
0

which is the same as:

>>> D['x'] if 'x' in D else 0
0

To get a list of keys:

>>> list(D.keys())
['name', 'job', 'age']

There is a built-in sorted function that sorts the dictionary by its keys and returns a list:

>>> sorted(D)
['age', 'job', 'name']

Compared to the for loops, while loops generally take more code to write.

For loops work on all iterable objects. An object is iterable if it is either a physically stored sequence in memory, or an object that generates one item at a time in the context of an iteration operation (like a "virual" sequence). Both types of objects are iterable because they respond to the iter call with an object that advances when a call to next is made, and then raises an exception when the object finishes. Every tool that scans an object from left to right uses the iteration protocol. 

Here is another example using comprehension that uses for and returns a list:

>>> squares = [x*x for x in [1,2,3,4,5]]
>>> squares
[1, 4, 9, 16, 25]

This is the same as:

>>> squares = []
>>> for x in [1,2,3,4,5]:
...     squares.append(x*x)
...
>>> squares
[1, 4, 9, 16, 25]

It is likely though not guaranteed that the comprehension syntax will run faster than explicitly typing out the sequence in with a for loop. A rule of thumb is to code for readability and simplicity first, and then optimization afterwards. The for loop version also allows for multiline statements and expressions. Oftentimes, the simple and readable methods are fast enough for your needs. 

Tuples are immutable sequences and are useful for larger programs where it's useful to constrain such data types. Otherwise, they are not as common as other sequences such as lists and arrays. You can't use the append method on tuples, for instance. You can create a new tuple from an old tuple like so:

>>> T = (1,2,3,4)
>>> T = (2,) + T[1:]
>>> T
(2, 2, 3, 4)

To create a text output file, pass it's name and the 'w' processing mode into the open built-in function:

>>> f = open('data.txt', 'w')
>>> f.write("Hello\n')
>>> f.close()

To read the file, use the 'r' processing mode, then read the content into a string:

>>> f = open('data.txt', 'r')
>>> text = f.read()
>>> print(text)

Files provide an iterator that can read line by lin in for loops:

>>> for line in open('data.txt'): print(line)

Binary files usually include media files or data created by C programs, etc... Python has a struct module that can create and unpack packed binary data - raw bytes that record values that are not Python objects. 

Python text automatically endcodes on writes and decodes on reads. 

You can encode and decode manually if processing from a Unicode data source oter than a file, such as email or data over network. 

Sets are also immutable objects, and are an unordered collection. They support the usual mathematical set operations. They are useful for determine unique elements, isolating differences, order-neutral equality tests, etc...

Order-neutral equality test:
>>> set('spam') == set('pmsa')
True

The type built-in function returns the object's class. A few ways to do type testing:

type(L) == type([])

type(L) == list

isinstance(L, list)

However, this is usually the wrong thing to do in Python, and is usually a sign of an ex-C programmer (haha - author's words, not mine). The idea is that if you do type testing, you limit your flexibility. I bet this is controversial (that last bit was my words, not the authors). 

It is related to the idea of polymorphism. In Python, one codes to object interfaces, not to types. We care about what an object does, not what it is. Any object with a compatible interface should work, regardless of its specific type. 

We extend software by writing new classes, not by changing what already works. 

14-feb-2018
===========

Chapter 5 - Numberic Types

Data takes the form of objects. Numeric types include integeres, floating-point, complex number, and fixed-precision (decimal), fractions, ets, booleans, built-in functions and modules (pow, abs, round, math, random), expressions (unlimited integer precision, bitwise opeations, hex, octal, and binary), third-party extensions such as vectors, libraries, visualization, and plotting, objects.

Integers can have unlimited precision, and grow to as many digits as the memory allows. Integers are written as strings of decimal digits.

Built-in functions hex(I), oct(I), and bin(I) convert an integer to its representation string in these three bases. int(str, base) also converts to a specific base. 

There are type-specific methods. For floating-point numbers, there's an "is_integer" method to test if the number is an integer, or "as_integer_ratio" to show the fraction number type. "bit_length" gives the number of bits necessary to represent the number.

16-feb-2018
===========

The "is" operator tests for value quality but also address in memory.

The "lambda" creates an unnamed function.

Some python expressions:

yield x => Generator function send protocol
lambda args: expression => anonymous function generation
x if y else z
x | y => bitwise OR, set union
x ^ y = bitwise XOR, set symmetric differences
x & y => bitwise AND, set intersection
x // y => floor 
~x => bitwise NOT (inversion)
etc... 

When using [...], it's a list literal or a list comprehension, which is implied loop.

X < Y < Z is the same as X < Y and Y < Z.

Mixed operators follow order of operations, but it's just probably best practice to include paranthesis for best practices.

If adding two different numeric types, like an integer and a floating-point number, the result will be the more "complicated" one. Python will convert both types to the more complicated of the two, and the result will be the complicated type.

This automatic conversion usually just happens between numeric types - a string and integer can't be added together, for instance.

All Python operators can be overloaded by Python classes. For instance, classes can be added with x+y expressions, or indexed with x[i], etc... This is a property called polymorphism. It means that the definition of an operator depends on the types it is dealing with.

Variables are created when they are first assigned values. They are replaced with their values when used in an expression. They have to be assigned before being used in expressions. Variables refer to objects and are never declared ahead of time. Unlike in languages like C or Java, you don't have to predeclare variables in Python before using them - they just have to be assigned.

This is unexpected, but is due to limitations of number of bits where floating-point numbers cannot represent some values:

>>> 1.1 + 2.2 == 3.3
False


3-march-2018
============

In Python 3, dividing two integers keeps the remainder. For instance,

>>> 10/4
2.5

In Python 2, you can use the __future__ importer to get some functionality that is available in Python 3, such as the division operator returning remainders on integers. Normally, Python 2 would return an integer floor.

One thing to note is that it is a floor division, not a truncating division. That means, flooring a negative number will round it down:

>>> math.floor(-2.5)
-3

If you want to force the floor, use // instead.

If truncation is truly desired, use the math.trunc method.

This difference between Python 3 and Python 2 may break some Python 2 programs. Many programmers rely on division truncation because of the C language legacy. 

When integers grow large, integer math operations take a little longer because Python does some extra work to support extended precision. 

It is simple to change an integer's base to binary, octal, or hexadecimal. In all of these cases, the integer's value is the same in memory. For instance, the octal value 003777, the hex value 0xFF, and binary value 0b11111111 are all decimal 255, and they all are the same in value memory.

To convert from decimal to other bases, one can use:
oct(64), hex(64), or bin(64).

The eval function treats strings as if they are Python code. It compiles and runs the string as a piece of the program. Care should be taken with this call because it can be used to overtake a machine.

Bitwise operators:

| bitwise OR
& bitwise AND
<< shift left
^ bitwise XOR (either but not both)

The general idea is that at a high-level language like Python, you're not likely to be dealing in bits. If so, you may need to question if you're using the right language, or if you should be using something like C.

Here are some notes about the random module:

>>> import random
>>> random.random()
0.045110277849689795
>>> random.random()
0.7275828610097722
>>> random.randint(1, 10)
3

It can also choose an item at random:

>>> suits = ['heart', 'spades', 'diamonds', 'clubs']
>>> random.choice(suits)
'heart'

>>> random.shuffle(suits)
>>> suits
['clubs', 'diamonds', 'heart', 'spades']

Decimal types are like floating-point types except they have a fixed number of decimal points. They are fixed-precision floating-point values. We can specify how many digits. It is good for better numeric accuracy because it represents fixed-precision values but keeps the accuracy as part of the characteristics of its type. In contrast, floating-point values have limited space to store values.

To illustrate the point above, here's an example:

>>> 0.1 + 0.1 + 0.1 - 0.3
5.551115123125783e-17
>>> from decimal import Decimal
>>> Decimal('0.1') + Decimal('0.1') + Decimal('0.1') - Decimal('0.3')
Decimal('0.0')

Fraction types keep the numerator and denominator, and can avoid the inaccuracies and limitations of floating-point math. Their performance may not be as good because they do not map as closely to computer hardware as floating-point numbers. Here is an example in action:

>>> from fractions import Fraction
>>> x = Fraction(1, 3)
>>> y = Fraction(4, 6)
>>> x + y
Fraction(1, 1)

Sets - an item only appears once in a set, by definition. 

4-march-2018
============

Sets are also unordered and do not map keys to values.

Here are some examples of using sets:

>>> x = set('abcde')
>>> y = set('bdxyz')
>>> x
set(['a', 'c', 'b', 'e', 'd'])
>>> x - y
set(['a', 'c', 'e'])

Sets are iterable containers, so they can be used in operations like len, for loops, and list comprehensions.

There are also method-based counterparts:

>>> S = set([1,2,3])
>>> S.union([3,4])
set([1, 2, 3, 4])
>>> S.issubset(range(-5,5))
True

Python's built-in sets use efficient algorithms and implementation techniques.

In Python 3, you can also create a set with just curly brackets:

>>> T = {1,2,3,4}
>>> T
set([1, 2, 3, 4])

The community thought the syntax makes sense, because essentially sets are valueless dictionaries! A set's items are unordered, unique, and immunitable - much like a keys in a dictionary.

Sets can only contain immutable (hashable) obejct types. Hence, lists and dictionaries can't be embedded in sets.

Example of set list comprehension:

>>> {x ** 2 for x in [1, 2, 3, 4]}
set([16, 1, 4, 9])

Sets can be used to filter duplicates out of other collections. To remove duplicates from a collection, you can convert the collection to a set and then convert it back again:

>>> L = [1, 2, 1, 3, 2, 4, 5]
>>> set(L)
set([1, 2, 3, 4, 5])
>>> L = list(set(L))
>>> L
[1, 2, 3, 4, 5]

Sets can be used to isolate differences - convert to sets and take the difference, although we will lose the information about the number of times an element occurred. 

Sets are equal if and only if every element of each set is contained in the other. This might be useful when you don't need to order something before comparing the results, since sets don't rely on an expensive sort. 

Sets are also covenient when dealing with large data sets.

Some might argue that the Boolean type is numeric in nature because it is just customized version of the integers 1 and 0.

The bool type has values True and False, which are preassigned built-in names. 

Here's something interesting:

>>> True + 4
5

Numeric extensions such as NumPy can offer advanced numeric tools, such as matrix data type, vector processing, and sophisticated computation libraries. Python with NumPy can get good performance with Python language and its libraries. 

12-march-2018
=============

Chapter 6

You don't declare variables ahead of time - types are determined at runtime.

A variable is created when it is first assigned a value. The notion of the type is with the object, not the names. Variables always refer to a particular object at a particular point in time. Variables must be assigned before they can be used. 

When you go a = 3, Python does these steps:

1) Create an object to represent the value 3.

2) Create the variable a, if it does not yet exist.

3) Link the variable a to the new object 3.

Variables always link to other objects and never to other variables. Objects may link to other objects, such as a list object can have links to the objects it contains. In the example above, variable a becomes a reference to the object 3. The variable is a pointer to the object's memory space that is created by running the literal expression 3.

Variables are entries in a system table, with spaces for links to objects.

Objects are pieces of allocated memory, with enough space to represent the values for which they stand.

References are automatically followed pointers from variables to objects.

For optimization, Python internally caches and resuses certain kinds of unchangeable objects, such as small integers and strings. Each object is a distinct piece of memory, and each expression's result value is a distinct object. 

Each object has more space than to just represent their values - they have two standard header fields - a type designator to mark the type of the object, and a reference counter, to say when it's possible to reclaim the object. 

Consider the following example:

a = 3
a = 'spam'
a = 1.23

Traditionally, it appears like the type of a is changing from integer to string to floating-point number. However, names like a have no types. Types live with the objects. a is just changing its reference to different objects. We haven't changed the type of the variable a, we've made the variable reference a differet type of object. All we can say about a variable is that it represents a particular object at a particular point in time. 

Objects, as mentioned before, contains a header field that tags the object with its type. For instance, the integer object 3 will contain a designator that tells Python that the object is an integer, or in other words, will contain a pointer to an object called int - the name of the integer type. 'spam' has type designator that points to the string type called str. Objects know their types, so variables don't have to.

When a name (or variable) is assigned to a new object, the space held by the prior object is reclaimed as long as it is not references by another name or object. This automatic reclaimation is garbage collection. 

Reclamation happens when the reclaim counter drops to zero - everytime an variable references that object, the reclaim counter increments. When the object no longer as a variable referencing it, the reclaim counter goes to 0, and the object's memory space is reclaimed. 

Consider the following scenario, known as a shared reference, or a shared object:

a = 3
b = a

A variable can't link to another variable - only to an object. Thus, a and b are both referencing the object 3. b is really a pointer to the objec'ts memory space created by running the literal expression 3.

Now consider this:

a = 3
b = a 
a = 'spam'

In the end here, b references 3 still, and a references a new object, 'spam'. Remember, a variable can never reference another variable. 

Python variables are always pointers to objects. Setting a variable to a new value causes the variable to reference an entirely different object. 

There are objects and operations that perform in-place object changes - which are Python's mutable types. These include lists, dictionaries, and sets. This is where shared references can make things less trivial.

For instance, a change from one name may impact others. Objects may seem to change for no apparent reason. All assignments are based on references, including function argument passing.  

The following example illustrates the point:

L1 = [2, 3, 4]
L2 = L1
L1[0] = 24

L1 is [24, 3, 4], but so is L2!

We haven't changed L1 - just a component of the object that L1 references. Since L2 references the same object, L2 will also be pointing to that changed list.

21-march-2018
=============

If you don't want this behavior, you can ask Python to copy the objects instead of making references. There are many ways to copy a list:

>>> L1 = [2,3,4]
>>> L2 = L1[:]
>>> L1[0] = 24
>>> L1
[24, 3, 4]
>>> L2
[2, 3, 4]

Doing the slicing makes L2 reference a copy of the object that L1 references, and not the original object. Slicing-copy technique only works with lists, and not sets or dictionaries. For those other, you can use the copy method, or pass the original boject to their type names, like dict or set.

The standard copy library:

>>> import copy
>>> X = copy.copy(Y) # -- top-level "shallow" copy of any object Y
>>> X = copy.deepcopy(Y) # -- deep copy of any object Y: copy all nested parts

Objects that can be changed in place (mutable) are open to these kind of reference assignment gotchas. Such objects include lists, dictionaries, sets, and other objects defined with class statements. 

Revisting garbage collection, there are some objects that aren't reclaimed, such as the literal small integer object 42, which is going to remain in a system table to be reused next time a 42 is generated in the code. Most objects are reclaimed immediately though. 

Because of the reference model, there are two different ways to check for equality:

>>> L = [1, 2, 3]
>>> M = L
>>> L == M
True
>>> L is M
True

The == technique tests whether the two referenced objects have the same values. This is the method used for equality checks. The 'is' technique tests for object identity, and returns True of both names point to exactly the same object. 'is' is a much stronger form of equality testing and is rarely applied in most programs. Example:

>>> L = [1, 2, 3]
>>> M = [1, 2, 3]
>>> L == M
True
>>> L is M
False

But small integers and strings are cached and reused, so they can point to the same object even though you might assign a different variable. See here:

>>> X = 42
>>> Y = 42
>>> X == Y
True
>>> X is Y
True

You can ask Python how many references there are to an object, using getrefcount.

>>> import sys
>>> sys.getrefcount(1)
747

747 pointers to 1.

This caching and reuse is lower level, and not typically relevant to your code (unless you are often using 'is'!) Numbers and strings are immutable, so it doesn't matter how many references there are to the same object, every reference is to the same unchanging value. This is how Python its model optmizes for speed.

The key pont of this chapter is that a variable can never reference another variable, only an object!

